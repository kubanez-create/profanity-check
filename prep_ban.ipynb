{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing step\n",
    "We are going to build a binary classifier checking user's input whether it's profanity or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ht9IKXgDcyuh"
   },
   "outputs": [],
   "source": [
    "#Preprocessing step\n",
    "#Let's make a corpus out of (roughly) 5000 profane words\n",
    "#and equal amount the most frequent russian words\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u57ON2L_eT_y"
   },
   "outputs": [],
   "source": [
    "#loading and (preliminary) cleaning set of profane words\n",
    "\n",
    "outfile = []\n",
    "alphabet = 'йцукенгшщзхъфывапролджэячсмить.*_@\" '\n",
    "with open('bad_words_corpus.txt', 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        if len(line) > 30:\n",
    "            for w in line.split():\n",
    "                news = ''\n",
    "                for l in range(len(w)):    #normalize each word\n",
    "                    if w[l] in alphabet:\n",
    "                        news += w[l]\n",
    "                if len(news) == 1:\n",
    "                    continue\n",
    "                outfile.append(news)\n",
    "        news = ''\n",
    "        for l in range(len(w)):    #normalize each word\n",
    "            if w[l] in alphabet:\n",
    "                news += w[l]\n",
    "        if len(news) == 1:\n",
    "            continue\n",
    "        outfile.append(line.strip())\n",
    "\n",
    "out = pd.Series(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqohPWOVeV0n"
   },
   "outputs": [],
   "source": [
    "# loading frequent words dict\n",
    "\n",
    "freq_words = pd.read_excel('freq_words_rus.xlsx', squeeze=True)\n",
    "\n",
    "#let's make a corpus\n",
    "\n",
    "cor = pd.concat((out,freq_words), ignore_index=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nabFj7ZeYKC"
   },
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(data, n_samples, l_sample):\n",
    "    '''\n",
    "    Function returns bootstrapped samples.\n",
    "    data - must be a numpy array\n",
    "    n_samples - number of samples, an integer\n",
    "    l_sample - max length of a sample\n",
    "    \n",
    "    '''\n",
    "    indices = np.random.randint(0, len(data), (n_samples, l_sample))\n",
    "    samples = data[indices]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a corpus out of a combined array consisting of 5000 most frequent russian words and roughly the same amount adult language. Then taking an array of random samples lasting from 1 word till 19 inclusively.\n",
    "Key problem is there's not a single open labeled dataset consisting out of adult and common language in russian. To circumvent it I've decided to make a synthetic one taking first 50 000 one-word samples from our combined array, then two-word samples and so on up to 19. Thus I get not so small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "C3vZez-EeZwA",
    "outputId": "cf2822e9-6e4a-4974-a393-0c1088a2a5f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_size = 50000\n",
    "for epoch in range(1, 20):\n",
    "    if epoch < 19:\n",
    "        if epoch == 1:\n",
    "            corp = get_bootstrap_samples(cor, sample_size, epoch).reshape(sample_size, 1)\n",
    "            length = 19 - epoch\n",
    "            mid = np.hstack((np.zeros((sample_size, length)), corp))\n",
    "            corpt = mid                                         # initializing final corpus \n",
    "    \n",
    "        corp = get_bootstrap_samples(cor, sample_size, epoch)\n",
    "        length = 19 - epoch\n",
    "        mid = np.hstack((np.zeros((sample_size, length)), corp))\n",
    "        corpt = np.vstack((mid, corpt))                         #stacking samples\n",
    "\n",
    "    else:\n",
    "        corp = get_bootstrap_samples(cor, sample_size, epoch)\n",
    "        corpt = np.vstack((corp, corpt))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model.\n",
    "Making a training text array and target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6bE5a136ypI"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(corpt)\n",
    "data_truth = data.isin(outfile)  # check each str whether it has profane word in it\n",
    "data_truth['sum'] = data_truth.sum(axis=1)\n",
    "data_truth['label'] = data_truth['sum'].apply(lambda x: 0 if x < 1 else 1)\n",
    "y = data_truth['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7iVn2YtL4Ln"
   },
   "outputs": [],
   "source": [
    "texts = data.to_csv(header=None, index=False).strip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UORPqegXROCB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joblib version is 0.16.0.\n",
      "The scikit-learn version is 0.23.1.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "\n",
    "import joblib\n",
    "import sklearn\n",
    "\n",
    "print('The joblib version is {}.'.format(joblib.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing our final data\n",
    "\n",
    "estimators = [('tfidf', TfidfTransformer()), ('svd', TruncatedSVD(1))]\n",
    "combined = FeatureUnion(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking accuracy of our model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.995345 0.999975 0.99997  0.99979  0.96129 ]\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(cross_val_score(\n",
    "    Pipeline([\n",
    "            (\"vectorizer\", CountVectorizer()),\n",
    "            (\"transformer\", combined),\n",
    "            (\"classifier\", LinearSVC())\n",
    "        ]),\n",
    "    texts,\n",
    "    y\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
       "                ('transformer',\n",
       "                 FeatureUnion(transformer_list=[('tfidf', TfidfTransformer()),\n",
       "                                                ('svd',\n",
       "                                                 TruncatedSVD(n_components=1))])),\n",
       "                ('classifier', LinearSVC())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building final model\n",
    "\n",
    "clf_pipeline2 = Pipeline([\n",
    "            (\"vectorizer\", CountVectorizer()),\n",
    "            (\"transformer\", combined),\n",
    "            (\"classifier\", LinearSVC())\n",
    "        ])\n",
    "\n",
    "\n",
    "clf_pipeline2.fit(texts, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying some simple tests and checking perfomance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1]\n",
      "Wall time: 8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(clf_pipeline2.predict(['пошел нахуй', 'Все хорошо!', 'Потому что у мужчин подогрев хуевый, а у женщин пиздатый', '...Я не такой дурак, как ты выглядишь...',\n",
    "            'моя фантазия заканчивается, ну допустим мудак']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model for deployment\n",
    "\n",
    "joblib.dump(clf_pipeline2, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "prep_ban.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
